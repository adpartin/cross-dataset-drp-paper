{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/ml_lab/projects/improve/data/experiments/cross-dataset-drp-paper\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap, Normalize\n",
    "\n",
    "import importlib\n",
    "import utils  # Import your module\n",
    "from utils import model_name_mapping, metrics_name_mapping\n",
    "\n",
    "# After making changes to utils.py, reload it\n",
    "importlib.reload(utils)\n",
    "\n",
    "# filepath = Path(__file__).parent\n",
    "filepath = Path(os.path.abspath(''))\n",
    "print(filepath)\n",
    "\n",
    "metrics_name_mapping = {\n",
    "    \"r2\": \"R²\",\n",
    "    \"mae\": \"MAE\",\n",
    "    \"rmse\": \"RMSE\",\n",
    "    \"stgr\": \"STGR\",\n",
    "    \"stgi\": \"STGI\",\n",
    "}\n",
    "\n",
    "model_name_mapping = {\n",
    "    \"deepcdr\": \"DeepCDR\",\n",
    "    \"graphdrp\": \"GraphDRP\",\n",
    "    \"hidra\": \"HiDRA\",\n",
    "    \"lgbm\": \"LGBM\",\n",
    "    \"tcnns\": \"tCNNS\",\n",
    "    \"uno\": \"UNO\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>met</th>\n",
       "      <th>split</th>\n",
       "      <th>value</th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mse</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>CCLE</td>\n",
       "      <td>CCLE</td>\n",
       "      <td>deepcdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mse</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>CCLE</td>\n",
       "      <td>CCLE</td>\n",
       "      <td>deepcdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mse</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>CCLE</td>\n",
       "      <td>CCLE</td>\n",
       "      <td>deepcdr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   met  split     value   src   trg    model\n",
       "0  mse      0  0.006295  CCLE  CCLE  deepcdr\n",
       "1  mse      1  0.005950  CCLE  CCLE  deepcdr\n",
       "2  mse      2  0.005129  CCLE  CCLE  deepcdr"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = Path('splits_averaged')\n",
    "outdir = filepath / 'results_for_paper_revision'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# file_format = 'eps'\n",
    "# file_format = 'jpeg'\n",
    "# file_format = 'png'\n",
    "file_format = 'tiff'\n",
    "dpi = 600\n",
    "\n",
    "filename = 'all_models_scores.csv'\n",
    "canc_col_name = 'improve_sample_id'\n",
    "drug_col_name = 'improve_chem_id'\n",
    "\n",
    "# datasets_order = ['CCLE', 'CTRPv2', 'GDSCv1', 'GDSCv2', 'gCSI']  # alphabetical order\n",
    "datasets_order = ['gCSI', 'CCLE', 'GDSCv2', 'GDSCv1', 'CTRPv2']  # order by sample size\n",
    "show_plot = True\n",
    "\n",
    "all_scores = pd.read_csv(filepath / datadir / filename, sep=',')\n",
    "all_scores.iloc[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the metric\n",
    "metric_name = \"r2\"\n",
    "\n",
    "# Specify the models you want to include\n",
    "models_to_include = []  # Replace with your desired models\n",
    "# models_to_include = [\"deepcdr\", \"graphdrp\", \"hidra\", \"lgbm\", \"uno\"]  # Replace with your desired models\n",
    "# models_to_include = [\"graphdrp\"]  # Replace with your desired models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests (Reviewer 3, comment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "\n",
    "# Filtering for R² scores\n",
    "r2_df = all_scores[all_scores['met'] == 'r2'][['src', 'trg', 'model', 'split', 'value']]\n",
    "r2_df['model'] = r2_df['model'].map(model_name_mapping)\n",
    "\n",
    "# Defining models and datasets\n",
    "models = r2_df['model'].unique()\n",
    "src_datasets = r2_df['src'].unique()\n",
    "trg_datasets = r2_df['trg'].unique()\n",
    "\n",
    "# Preparing results\n",
    "results = []\n",
    "skipped = []\n",
    "alpha = 0.05 / 15  # Bonferroni correction for 15 pairwise comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over source-target pairs\n",
    "for src in src_datasets:\n",
    "    for trg in trg_datasets:\n",
    "        # Data for this pair\n",
    "        pair_df = r2_df[(r2_df['src'] == src) & (r2_df['trg'] == trg)]\n",
    "        if pair_df.empty:\n",
    "            continue\n",
    "\n",
    "        model_pairs = combinations(models, 2)\n",
    "\n",
    "        # Pairwise Wilcoxon tests\n",
    "        for model1, model2 in model_pairs:\n",
    "            scores1 = pair_df[pair_df['model'] == model1]['value'].values\n",
    "            scores2 = pair_df[pair_df['model'] == model2]['value'].values\n",
    "            \n",
    "            if len(scores1) == 10 and len(scores2) == 10:  # Ensure 10 splits\n",
    "                try:\n",
    "                    stat, p = wilcoxon(scores1, scores2, alternative='two-sided')\n",
    "                    mean_diff = np.mean(scores1 - scores2)\n",
    "                    # if p < alpha:\n",
    "                    results.append({\n",
    "                        'src': src,\n",
    "                        'trg': trg,\n",
    "                        'model1': model1,\n",
    "                        'model2': model2,\n",
    "                        'median_model1': np.median(scores1),\n",
    "                        'median_model2': np.median(scores2),\n",
    "                        'mean_r2_diff': mean_diff,\n",
    "                        'p_value': p,\n",
    "                        'significant': p < alpha\n",
    "                    })\n",
    "                except:\n",
    "                    print(f\"Wilcoxon test failed for {model1} vs {model2} on {src} → {trg}: {e}\")\n",
    "            else:\n",
    "                print(f\"Skipping {model1} vs {model2} on {src} → {trg} due to insufficient splits: {len(scores1)} vs {len(scores2)}\")\n",
    "                skipped.append({\n",
    "                    'src': src,\n",
    "                    'trg': trg,\n",
    "                    'model1': model1,\n",
    "                    'model2': model2,\n",
    "                    'len_model1': len(scores1),\n",
    "                    'len_model2': len(scores2)\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon statistical tests completed. Results saved to 'wilcoxon_tests_r2_all_combos.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Saving significant results\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(outdir / 'wilcoxon_tests_r2_all_combos.csv', index=False)\n",
    "\n",
    "# Generating boxplots for key pairs\n",
    "for src in src_datasets:\n",
    "    for trg in trg_datasets:\n",
    "        pair_df = r2_df[(r2_df['src'] == src) & (r2_df['trg'] == trg)]\n",
    "        res_df_src_trg_combo = res_df[(res_df['src'] == src) & (res_df['trg'] == trg)]\n",
    "        res_df_src_trg_combo.to_csv(outdir / f'wilcoxon_tests_r2_{src}_{trg}_combos.csv', index=False)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # sns.boxplot(x='model', y='value', data=pair_df, palette='Set3')\n",
    "        # sns.boxplot(x='model', y='value', data=pair_df, hue='model', palette='Set3', legend=False)\n",
    "        sns.boxplot(x='model', y='value', data=pair_df, hue='model', palette='Set3', legend=False,\n",
    "            showmeans=True, meanprops={\"marker\":\"o\", \"markerfacecolor\":\"black\"})\n",
    "        plt.title(f'R² Scores: {src} → {trg}')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('R²')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(outdir / f'boxplot_{src}_{trg}.png')\n",
    "        plt.close()\n",
    "\n",
    "print(\"Wilcoxon statistical tests completed. Results saved to 'wilcoxon_tests_r2_all_combos.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bubble Heatmap (Reviewer 3, comment 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within-study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all within-study results (src == trg)\n",
    "df = all_scores[\n",
    "    (all_scores[\"met\"] == metric_name) & \n",
    "    (all_scores[\"src\"] == all_scores[\"trg\"])  # src == trg\n",
    "].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df = df.groupby([\"model\", \"src\"]).agg(mean_splits=(\"value\", \"mean\"), std_splits=(\"value\", \"std\")).reset_index()\n",
    "\n",
    "df_mean = df.sort_values(by=[\"src\", \"mean_splits\"], ascending=[True, False]).reset_index(drop=True)  # compute mean\n",
    "df_std = df.sort_values(by=[\"src\", \"std_splits\"], ascending=[True, False]).reset_index(drop=True)    # compute std\n",
    "# display(df.iloc[:7,:])\n",
    "\n",
    "# Mean across splits\n",
    "df_mean = df_mean.pivot(index=\"src\", columns=\"model\", values=\"mean_splits\")#.reset_index(drop=False)\n",
    "df_mean.index.name = None\n",
    "df_mean.columns.name = None\n",
    "df_mean = df_mean.T\n",
    "df_mean = df_mean.round(3)\n",
    "df_mean.index = df_mean.index.map(model_name_mapping)\n",
    "df_mean = df_mean[datasets_order]\n",
    "print('Mean across splits')\n",
    "display(df_mean)\n",
    "\n",
    "# Std across splits\n",
    "df_std = df_std.pivot(index=\"src\", columns=\"model\", values=\"std_splits\")#.reset_index(drop=False)\n",
    "df_std.index.name = None\n",
    "df_std.columns.name = None\n",
    "df_std = df_std.T\n",
    "df_std = df_std.round(3)\n",
    "df_std.index = df_std.index.map(model_name_mapping)\n",
    "df_std = df_std[datasets_order]\n",
    "print('Std across splits')\n",
    "display(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean across splits')\n",
    "display(df_mean)\n",
    "\n",
    "# Add new row to df_mean containing the mean of each column\n",
    "datasets_mean = df_mean.mean(axis=0)\n",
    "df_mean.loc['Mean across datasets'] = datasets_mean\n",
    "\n",
    "# Add new column to df_mean containing the mean of each row\n",
    "models_mean = df_mean.mean(axis=1)\n",
    "df_mean['Mean across models'] = models_mean\n",
    "\n",
    "# Assign NA to cell of (mean_dataset, mean_model)\n",
    "df_mean.loc['Mean across datasets', 'Mean across models'] = np.nan\n",
    "df_mean.to_csv(outdir / f'{metric_name}_mean_within_study_all_models.csv')\n",
    "\n",
    "print('Mean across splits (including across models and datasets)')\n",
    "display(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Std across splits')\n",
    "display(df_std)\n",
    "\n",
    "# Add new row to df_mean containing the mean of each column\n",
    "datasets_std = df_std.mean(axis=0)\n",
    "df_std.loc['Mean across datasets'] = datasets_std\n",
    "\n",
    "# Add new column to df_mean containing the mean of each row\n",
    "models_std = df_std.mean(axis=1)\n",
    "df_std['Mean across models'] = models_std\n",
    "\n",
    "# Assign NA to cell of (mean_dataset, mean_model)\n",
    "df_std.loc['Mean across datasets', 'Mean across models'] = np.nan\n",
    "df_std.to_csv(outdir / f'{metric_name}_std_within_study_all_models.csv')\n",
    "\n",
    "print('Std across splits (including across models and datasets)')\n",
    "display(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Generalization scores (violin) from a single source to all targets (src != trg)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "if len(models_to_include) == 0:\n",
    "    models_to_include = all_scores[\"model\"].unique()\n",
    "\n",
    "filtered_data = all_scores[\n",
    "    (all_scores[\"model\"].isin(models_to_include)) & \n",
    "    (all_scores[\"met\"] == metric_name) & \n",
    "    (all_scores[\"src\"] == all_scores[\"trg\"])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Map the model names to their corresponding names using model_name_mapping\n",
    "filtered_data['model'] = filtered_data['model'].map(model_name_mapping)\n",
    "\n",
    "utils.boxplot_violinplot_within_study(\n",
    "    df=filtered_data,\n",
    "    metric_name=metric_name,\n",
    "    models_to_include=models_to_include,\n",
    "    outdir=outdir,\n",
    "    file_format=file_format,\n",
    "    dpi=dpi,\n",
    "    ymin=0.35,\n",
    "    ymax=0.9,\n",
    "    datasets_order=datasets_order\n",
    ")\n",
    "\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Generalization scores (boxplot) from a single source to all targets (src != trg)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Specify the source dataset for filtering\n",
    "source_dataset = \"CTRPv2\"  # Replace with the specific source dataset\n",
    "\n",
    "if len(models_to_include) == 0:\n",
    "    models_to_include = all_scores[\"model\"].unique()\n",
    "\n",
    "# Filter data for specific source dataset (src = CTRPv2), and R^2 metric\n",
    "filtered_data = all_scores[\n",
    "    (all_scores[\"src\"] == source_dataset) &\n",
    "    (all_scores[\"src\"] != all_scores[\"trg\"]) &  # Exclude cases where src = trg\n",
    "    (all_scores[\"met\"] == metric_name) &\n",
    "    (all_scores[\"model\"].isin(models_to_include))  # Include only specified models\n",
    "]\n",
    "\n",
    "# Map the model names to their corresponding names using model_name_mapping\n",
    "filtered_data['model'] = filtered_data['model'].map(model_name_mapping)\n",
    "\n",
    "utils.boxplot_violinplot_cross_study(\n",
    "    df=filtered_data, \n",
    "    source_dataset=source_dataset, \n",
    "    metric_name=metric_name, \n",
    "    models_to_include=models_to_include, \n",
    "    outdir=outdir,\n",
    "    file_format=file_format,\n",
    "    dpi=dpi,\n",
    "    ymin=-0.15,\n",
    "    ymax=0.7,\n",
    "    datasets_order=[x for x in datasets_order if x != source_dataset]\n",
    ")\n",
    "\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw G matrices for all models (including std in parentheses) (Linear Scale)\n",
    "\n",
    "G_palette = 'Blues'\n",
    "\n",
    "G_mean_dfs = {}\n",
    "G_std_dfs = {}\n",
    "\n",
    "for model_name in model_name_mapping.keys():\n",
    "    print(model_name)\n",
    "    mean_csa_filename = f'{model_name}_{metric_name}_mean_csa_table.csv'\n",
    "    std_csa_filename = f'{model_name}_{metric_name}_std_csa_table.csv'\n",
    "\n",
    "    G_mean = pd.read_csv(filepath / datadir / mean_csa_filename, sep=',')\n",
    "    G_std = pd.read_csv(filepath / datadir / std_csa_filename, sep=',')\n",
    "\n",
    "    G_mean.set_index(\"src\", inplace=True)\n",
    "    G_std.set_index(\"src\", inplace=True)\n",
    "\n",
    "    # Save csv for llm\n",
    "    G_mean.to_csv(outdir / f'{model_name}_{metric_name}_G_mean.csv')\n",
    "    G_std.to_csv(outdir / f'{model_name}_{metric_name}_G_std.csv')\n",
    "\n",
    "    G_mean_dfs[model_name] = G_mean\n",
    "    G_std_dfs[model_name] = G_std\n",
    "\n",
    "    utils.csa_heatmap(\n",
    "        model_name=model_name, \n",
    "        metric_name=f\"{metrics_name_mapping[metric_name]}\",\n",
    "        csa_metric_name='G',\n",
    "        scores_csa_data=G_mean, \n",
    "        std_csa_data=G_std,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        outdir=outdir,\n",
    "        file_format=file_format,\n",
    "        dpi=dpi,\n",
    "        palette=G_palette,\n",
    "        decimal_digits=3,\n",
    "        show=show_plot\n",
    "    )\n",
    "\n",
    "del model_name, G_mean, G_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a single G matrix for a specific model\n",
    "# model_id = 0\n",
    "# model_name = list(G_mean_dfs.keys())[model_id]\n",
    "\n",
    "# scores_csa_data = G_mean_dfs[model_name]\n",
    "# scores_csa_data.index.name = None\n",
    "# scores_csa_data.columns.name = None\n",
    "\n",
    "# std_csa_data = G_std_dfs[model_name]\n",
    "# std_csa_data.index.name = None\n",
    "# std_csa_data.columns.name = None\n",
    "\n",
    "# print(f'{model_name} r2 scores')\n",
    "# print(scores_csa_data)\n",
    "\n",
    "# print(f'{model_name} r2 stds')\n",
    "# print(std_csa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------------------------------------------\n",
    "# ## CSA Scores with Standard Deviations (Discrete Levels Same Color)\n",
    "# # --------------------------------------------------------------------------------------\n",
    "\n",
    "# # Define discrete levels and custom colormap\n",
    "# levels = [-1e6, 0, 0.25, 0.5, 0.7, 1]\n",
    "# colors = [\"#08306b\", \"#2171b5\", \"#6baed6\", \"#bdd7e7\", \"#eff3ff\"]\n",
    "# cmap = ListedColormap(colors)\n",
    "# norm = BoundaryNorm(boundaries=levels, ncolors=len(colors))\n",
    "\n",
    "# # Combine scores and stds for annotations\n",
    "# combined_annotations = scores_csa_data.round(4).astype(str) + \"\\n(\" + std_csa_data.round(4).astype(str) + \")\"\n",
    "\n",
    "# # Plot the combined heatmap\n",
    "# plt.figure(figsize=(7, 5))\n",
    "# sns.heatmap(\n",
    "# scores_csa_data, \n",
    "#     annot=combined_annotations.values, \n",
    "#     fmt=\"\", \n",
    "#     cmap=cmap, \n",
    "#     norm=norm, \n",
    "#     cbar_kws={'label': 'R² Score'}\n",
    "# )\n",
    "\n",
    "# # Customize colorbar ticks to align with levels\n",
    "# colorbar = plt.gca().collections[0].colorbar\n",
    "# colorbar.set_ticks(levels[1:])  # Exclude the placeholder -1e6\n",
    "# colorbar.set_ticklabels([\"< 0\", \"0-0.25\", \"0.25-0.5\", \"0.5-0.7\", \"> 0.7\"])  # Custom labels\n",
    "\n",
    "# plt.title(\"CSA Performance Scores with Standard Deviations (Discrete Levels)\")\n",
    "# plt.xlabel(\"Target Dataset\")\n",
    "# plt.ylabel(\"Source Dataset\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # --------------------------------------------------------------------------------------\n",
    "# ## CSA Scores with Standard Deviations (Discrete Levels Different Colors)\n",
    "# # --------------------------------------------------------------------------------------\n",
    "\n",
    "# # Define custom levels and pale colors\n",
    "# levels = [-1e6, 0, 0.25, 0.5, 0.7, 1]  # Replace -float(\"inf\") with a very small value\n",
    "# colors = [\"#dcd0ff\", \"#ffd1d1\", \"#ffebcc\", \"#ffffcc\", \"#d1ffd1\"]  # Pale purple, red, orange, yellow, green\n",
    "# cmap = ListedColormap(colors)\n",
    "# norm = BoundaryNorm(boundaries=levels, ncolors=len(colors))\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(7, 5))\n",
    "# sns.heatmap(scores_csa_data, annot=True, fmt=\".2f\", cmap=cmap, norm=norm, cbar_kws={'label': 'R² Score'})\n",
    "\n",
    "# # Customize colorbar ticks to align with levels\n",
    "# colorbar = plt.gca().collections[0].colorbar\n",
    "# colorbar.set_ticks(levels[1:])  # Exclude the placeholder -1e6\n",
    "# colorbar.set_ticklabels([\"< 0\", \"0-0.25\", \"0.25-0.5\", \"0.5-0.7\", \"> 0.7\"])  # Custom labels\n",
    "\n",
    "# # Finalize plot\n",
    "# plt.title(\"CSA Performance Scores with Standard Deviations (Discrete Levels)\")\n",
    "# plt.xlabel(\"Target Dataset\")\n",
    "# plt.ylabel(\"Source Dataset\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom metrics (Ga, Gn, Gna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scores from a single model\n",
    "\n",
    "# # Example CSA scores (R²)\n",
    "# scores_csa_data = {\n",
    "#     \"CCLE\": [0.7479, 0.5758, 0.4482, 0.3082, 0.0234],\n",
    "#     \"CTRPv2\": [-0.4671, 0.8508, -0.1432, -0.0324, -1.1319],\n",
    "#     \"GDSCv1\": [-0.2057, 0.1503, 0.734, 0.154, -0.4801],\n",
    "#     \"GDSCv2\": [-0.2913, 0.2902, 0.2003, 0.7659, -0.7147],\n",
    "#     \"gCSI\": [-0.3793, 0.2314, 0.4179, 0.3914, 0.733],\n",
    "# }\n",
    "# scores_csa_data = pd.DataFrame(scores_csa_data, index=[\"CCLE\", \"CTRPv2\", \"GDSCv1\", \"GDSCv2\", \"gCSI\"])\n",
    "\n",
    "# # Example CSA std deviations (optional for some metrics)\n",
    "# std_csa_data = {\n",
    "#     \"CCLE\": [0.0123, 0.0406, 0.0245, 0.0364, 0.1058],\n",
    "#     \"CTRPv2\": [0.0249, 0.0031, 0.0135, 0.0253, 0.0901],\n",
    "#     \"GDSCv1\": [0.078, 0.0301, 0.0065, 0.0389, 0.0304],\n",
    "#     \"GDSCv2\": [0.0453, 0.0216, 0.0262, 0.0098, 0.0448],\n",
    "#     \"gCSI\": [0.1053, 0.1546, 0.0903, 0.1144, 0.0314],\n",
    "# }\n",
    "# std_csa_data = pd.DataFrame(std_csa_data, index=[\"CCLE\", \"CTRPv2\", \"GDSCv1\", \"GDSCv2\", \"gCSI\"])\n",
    "\n",
    "model_name = \"graphdrp\"\n",
    "\n",
    "mean_csa_filename = f'{model_name}_{metric_name}_mean_csa_table.csv'\n",
    "std_csa_filename = f'{model_name}_{metric_name}_std_csa_table.csv'\n",
    "\n",
    "scores = pd.read_csv(filepath / datadir / mean_csa_filename, sep=',')\n",
    "stds = pd.read_csv(filepath / datadir / std_csa_filename, sep=',')\n",
    "\n",
    "scores.set_index(\"src\", inplace=True)\n",
    "stds.set_index(\"src\", inplace=True)\n",
    "\n",
    "print(f'Model: {model_name}')\n",
    "display(scores)\n",
    "display(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ga matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Ga using both implementations\n",
    "Ga_bruteforce = utils.compute_aggregated_G_bruteforce(scores, normalize=False)\n",
    "Ga_vectorized = utils.compute_aggregated_G_vectorized(scores, normalize=False)\n",
    "\n",
    "# Compare results\n",
    "print(f\"Bruteforce Ga:\\n{Ga_bruteforce}\")\n",
    "print(f\"Vectorized Ga:\\n{Ga_vectorized}\")\n",
    "\n",
    "# Check if all implementations are consistent\n",
    "assert Ga_vectorized == Ga_bruteforce, \"Mismatch between bruteforce and vectorized implementations!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Ga scores from all models\n",
    "Ga_list = []\n",
    "\n",
    "for model_name in model_name_mapping.keys():\n",
    "    print(model_name)\n",
    "    \n",
    "    mean_csa_filename = f'{model_name}_{metric_name}_mean_csa_table.csv'\n",
    "    scores_csa_data = pd.read_csv(filepath / datadir / mean_csa_filename, sep=',')\n",
    "    scores_csa_data.set_index('src', inplace=True)\n",
    "    Ga = utils.compute_aggregated_G_vectorized(scores_csa_data, normalize=False)\n",
    "\n",
    "    Ga['model'] = model_name\n",
    "    Ga_list.append(Ga)\n",
    "\n",
    "Ga_df = pd.DataFrame(Ga_list)\n",
    "Ga_df.insert(loc=0, column='model', value=Ga_df.pop('model')) # place 'model' col at pos 0\n",
    "Ga_df.set_index('model', inplace=True)\n",
    "Ga_df.to_csv(outdir / 'Ga_table.csv')\n",
    "print(Ga_df)\n",
    "\n",
    "del model_name, scores_csa_data, Ga, Ga_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ga heatmap\n",
    "utils.aggregated_G_heatmap(\n",
    "    metric_name=f\"Aggregated {metrics_name_mapping[metric_name]}\",\n",
    "    csa_metric_name='Ga',\n",
    "    scores_aggregated_data=Ga_df.copy(),\n",
    "    # palette=\"RdPu\",\n",
    "    palette=G_palette,\n",
    "    vmin=0,\n",
    "    vmax=0.6,\n",
    "    outdir=outdir,\n",
    "    file_format=file_format,\n",
    "    dpi=dpi,\n",
    "    show=show_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gn matrix\n",
    "(this was previously Source-to-Target Generalization Ratio (STGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gn using both implementations\n",
    "Gn_bruteforce = utils.compute_Gn_bruteforce(scores)\n",
    "Gn_vectorized = utils.compute_Gn_vectorized(scores)\n",
    "\n",
    "# Convert bruteforce results to DataFrame for comparison\n",
    "Gn_bruteforce_df = pd.DataFrame.from_dict(Gn_bruteforce, orient=\"index\")\n",
    "\n",
    "# Compare results\n",
    "print(f'Bruteforce Gn:\\n{Gn_bruteforce_df}')\n",
    "print(f'\\nVectorized Gn:\\n{Gn_vectorized}')\n",
    "\n",
    "# Check for equality\n",
    "assert np.allclose(Gn_bruteforce_df, Gn_vectorized), \"Gn results do not match!\"\n",
    "print(\"\\nBoth implementations produce the same results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gn_palette = 'Greens'\n",
    "\n",
    "for model_name in model_name_mapping.keys():\n",
    "    print(model_name)\n",
    "    \n",
    "    mean_csa_filename = f'{model_name}_{metric_name}_mean_csa_table.csv'\n",
    "    scores_csa_data = pd.read_csv(filepath / datadir / mean_csa_filename, sep=',')\n",
    "    scores_csa_data.set_index(\"src\", inplace=True)\n",
    "    Gn = utils.compute_Gn_vectorized(scores_csa_data)\n",
    "    Gn.to_csv(outdir / f'{model_name}_{metric_name}_Gn_mean.csv') # Save csv for llm\n",
    "\n",
    "    utils.csa_heatmap(\n",
    "        model_name=model_name, \n",
    "        # metric_name=\"stgr\",\n",
    "        metric_name=f\"Normalized {metrics_name_mapping[metric_name]}\",\n",
    "        csa_metric_name='Gn',\n",
    "        scores_csa_data=Gn, \n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        palette=Gn_palette,\n",
    "        outdir=outdir,\n",
    "        file_format=file_format,\n",
    "        dpi=dpi,\n",
    "        decimal_digits=3,\n",
    "        show=show_plot\n",
    "    )\n",
    "\n",
    "del model_name, scores_csa_data, Gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a single Gn matrix for a specific model\n",
    "# model_id = 5\n",
    "# model_name = list(G_mean_dfs.keys())[model_id]\n",
    "\n",
    "# scores_csa_data = G_mean_dfs[model_name]\n",
    "\n",
    "# Gn = utils.compute_Gn_vectorized(scores_csa_data)\n",
    "# Gn.index.name = None\n",
    "# Gn.columns.name = None\n",
    "\n",
    "# print(f'{model_name} Gn')\n",
    "# print(Gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gna matrix\n",
    "(previously Source-to-Target Generalization Index (STGI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gna using both implementations\n",
    "Gna_bruteforce = utils.compute_aggregated_G_bruteforce(scores, normalize=True)\n",
    "Gna_vectorized = utils.compute_aggregated_G_vectorized(scores, normalize=True)\n",
    "\n",
    "# Compare results\n",
    "print(\"Bruteforce Gna:\")\n",
    "print(Gna_bruteforce)\n",
    "\n",
    "print(\"\\nVectorized Gna:\")\n",
    "print(Gna_vectorized)\n",
    "\n",
    "# Check if all implementations are consistent\n",
    "# assert Gna_vectorized.equals(pd.Series(Gna_bruteforce)), \"Mismatch between bruteforce and simpler implementations!\"\n",
    "assert Gna_vectorized == Gna_bruteforce, \"Mismatch between bruteforce and vectorized implementations!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Gna scores from all models\n",
    "Gna_list = []\n",
    "\n",
    "for model_name in model_name_mapping.keys():\n",
    "    print(model_name)\n",
    "    \n",
    "    mean_csa_filename = f'{model_name}_{metric_name}_mean_csa_table.csv'\n",
    "    scores_csa_data = pd.read_csv(filepath / datadir / mean_csa_filename, sep=',')\n",
    "    scores_csa_data.set_index('src', inplace=True)\n",
    "    Gna = utils.compute_aggregated_G_vectorized(scores_csa_data, normalize=True)\n",
    "\n",
    "    Gna['model'] = model_name\n",
    "    # print(stgi)\n",
    "    Gna_list.append(Gna)\n",
    "\n",
    "Gna_df = pd.DataFrame(Gna_list)\n",
    "Gna_df.insert(loc=0, column='model', value=Gna_df.pop('model')) # place 'model' col at pos 0\n",
    "Gna_df.set_index('model', inplace=True)\n",
    "Gna_df.to_csv(outdir / 'Gna_table.csv')\n",
    "print(Gna_df)\n",
    "\n",
    "del model_name, scores_csa_data, Gna, Gna_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gna heatmap\n",
    "utils.aggregated_G_heatmap(\n",
    "    metric_name=f\"Aggregated normalized {metrics_name_mapping[metric_name]}\",\n",
    "    csa_metric_name='Gna',\n",
    "    scores_aggregated_data=Gna_df.copy(),\n",
    "    # palette=\"RdPu\",\n",
    "    palette=Gn_palette,\n",
    "    vmin=0,\n",
    "    vmax=0.6,\n",
    "    outdir=outdir,\n",
    "    file_format=file_format,\n",
    "    dpi=dpi,\n",
    "    show=show_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palette = \"RdGn\"\n",
    "\n",
    "# if palette in plt.colormaps():  # Check if the palette is a valid Matplotlib colormap\n",
    "#     cmap = plt.get_cmap(palette)\n",
    "# else:\n",
    "#     cmap = sns.color_palette(palette, as_cmap=True)  # Use Seaborn for custom palettes\n",
    "\n",
    "# print(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palette in plt.colormaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   available_colormaps = plt.colormaps()\n",
    "   print(available_colormaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source-to-Target Variability Ratio (STVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STVR (Source-to-Target Variability Ratio)\n",
    "# \"\"\"\n",
    "# The STVR quantifies the relative variability of a model’s performance for a \n",
    "# source-target pair by comparing the variability (standard deviation) of \n",
    "# predictions to the average performance for that pair. It is designed to provide \n",
    "# insight into the stability of predictions across different source-target combinations.\n",
    "\n",
    "# STVR evaluates stability per source-target pair, telling you how consistent or \n",
    "# erratic the model predictions are in that scenario.\n",
    "\n",
    "# Key Characteristics\n",
    "#     - Pairwise Metric: Evaluates the variability for each source-target combination,\n",
    "#         enabling fine-grained analysis of prediction stability.\n",
    "#     - Normalization: Normalizes the standard deviation of predictions by the mean \n",
    "#         performance, allowing for direct comparison across source-target pairs, \n",
    "#         regardless of scale.\n",
    "#     - Interpretation\n",
    "#         - > 1: Indicates high variability relative to the average performance, \n",
    "#             suggesting instability.\n",
    "#         - 0 < STVR < 1: Indicates low variability relative to the average performance, \n",
    "#             suggesting stability.\n",
    "#         - < 0: Reflects variability relative to poor performance\n",
    "#     - Caveats\n",
    "#         - Sensitive to both variability (numerator) and performance (denominator): \n",
    "#             Small mean performance values in the denominator can inflate the ratio,\n",
    "#             potentially misrepresenting variability.\n",
    "#         - Requires sufficient sample size for robust std computation.\n",
    "#     - Edge Cases\n",
    "#         - Zero mean performance: If mean_abs(src→trg)=0, assign a default value (e.g., 0) \n",
    "#             to avoid division by zero.\n",
    "\n",
    "# Formula:\n",
    "#     STVR[src][trg] = std_dev(src → trg) / mean_score(src → trg)\n",
    "#     Where:\n",
    "#         - std_dev(src→trg): Standard deviation of predictions for the source-target pair.\n",
    "#         - mean_score(src→trg): Mean performance for the source-target pair.\n",
    "# \"\"\"\n",
    "\n",
    "# def compute_stvr_vectorized(scores, stds):\n",
    "#     \"\"\"\n",
    "#     Compute STVR (Source-to-Target Variability Ratio) using a vectorized approach.\n",
    "\n",
    "#     Args:\n",
    "#         scores (pd.DataFrame): DataFrame where each cell contains a performance score.\n",
    "#         stds (pd.DataFrame): DataFrame where each cell contains the standard deviation \n",
    "#                              of scores for the corresponding source-target pair.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: A DataFrame with STVR values for each source-target pair.\n",
    "#     \"\"\"\n",
    "#     #mean_abs = scores.abs()  # Compute mean absolute score (element-wise)\n",
    "#     #stvr = stds / mean_abs.replace(0, np.nan)  # Compute STVR, avoiding division by zero\n",
    "#     stvr = stds / scores.replace(0, np.nan)  # Compute STVR, avoiding division by zero\n",
    "#     stvr = stvr.fillna(0)  # Replace NaN values with 0 for consistency\n",
    "#     return stvr\n",
    "\n",
    "\n",
    "# def compute_stvr_bruteforce(scores, stds):\n",
    "#     \"\"\"\n",
    "#     Compute STVR (Source-to-Target Variability Ratio) for each source-target pair.\n",
    "#     This implementation avoids vectorized operations.\n",
    "\n",
    "#     Args:\n",
    "#         scores (pd.DataFrame): DataFrame where each cell contains a performance score.\n",
    "#         stds (pd.DataFrame): DataFrame where each cell contains the standard deviation \n",
    "#                              of scores for the corresponding source-target pair.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: A DataFrame with STVR values for each source-target pair.\n",
    "#     \"\"\"\n",
    "#     stvr = pd.DataFrame(index=scores.index, columns=scores.columns)\n",
    "\n",
    "#     # Iterate through each source-target pair\n",
    "#     for src in scores.index:\n",
    "#         for trg in scores.columns:\n",
    "#             # Get mean absolute score and std deviation\n",
    "#             #mean_abs = abs(scores.loc[src, trg])\n",
    "#             mena_score = scores.loc[src, trg]\n",
    "#             std_dev = stds.loc[src, trg]\n",
    "            \n",
    "#             # Compute STVR\n",
    "#             #stvr.loc[src, trg] = std_dev / mean_abs if mean_abs != 0 else 0\n",
    "#             stvr.loc[src, trg] = std_dev / mena_score if mena_score != 0 else 0\n",
    "\n",
    "#     return stvr\n",
    "\n",
    "\n",
    "# stvr_bruteforce = compute_stvr_bruteforce(scores, stds).apply(pd.to_numeric)\n",
    "# stvr_vectorized = compute_stvr_vectorized(scores, stds).apply(pd.to_numeric)\n",
    "\n",
    "# print(stvr_bruteforce)\n",
    "# print(stvr_vectorized)\n",
    "\n",
    "# assert np.allclose(stvr_bruteforce.values, stvr_vectorized.values,\n",
    "#         atol=1e-6,  # Adjust the absolute tolerance for small rounding errors\n",
    "#         rtol=1e-5,   # Adjust the relative tolerance\n",
    "#         equal_nan=True), \"STVR results do not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source-to-Target Variability Index (STVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STVI (Source-to-Target Variability Index)\n",
    "# \"\"\"\n",
    "# STVI summarizes the variability of a model’s performance considering all \n",
    "# source-target pairs and normalizing the overall variability (standard deviation) \n",
    "# by the overall mean absolute performance.\n",
    "\n",
    "# STVI aggregates variability globally, helping you compare which models are more \n",
    "# stable across the board, but not telling you which datasets or source-target \n",
    "# pairs contribute to that stability or instability.\n",
    "# \"\"\"\n",
    "\n",
    "# def compute_stvi_vectorized(scores, stds):\n",
    "#     \"\"\"\n",
    "#     Compute STVI (Source-to-Target Variability Index) using a vectorized approach.\n",
    "\n",
    "#     Args:\n",
    "#         scores (pd.DataFrame): DataFrame where each cell contains a performance score.\n",
    "#         stds (pd.DataFrame): DataFrame where each cell contains the standard deviation \n",
    "#                              of scores for the corresponding source-target pair.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The STVI value for the entire CSA study.\n",
    "#     \"\"\"\n",
    "#     mean_abs = scores.abs().mean().mean()  # Compute overall mean absolute score\n",
    "#     overall_std_dev = stds.values.std()  # Compute overall standard deviation of stds\n",
    "\n",
    "#     # Compute STVI\n",
    "#     return overall_std_dev / mean_abs if mean_abs != 0 else 0\n",
    "\n",
    "\n",
    "# def compute_stvi_bruteforce(scores, stds):\n",
    "#     \"\"\"\n",
    "#     Compute STVI (Source-to-Target Variability Index) for the entire CSA study.\n",
    "#     This implementation avoids vectorized operations for a fully brute-force calculation.\n",
    "\n",
    "#     Args:\n",
    "#         scores (pd.DataFrame): DataFrame where each cell contains a performance score.\n",
    "#         stds (pd.DataFrame): DataFrame where each cell contains the standard deviation \n",
    "#                              of scores for the corresponding source-target pair.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The STVI value for the entire CSA study.\n",
    "#     \"\"\"\n",
    "#     all_scores = []\n",
    "#     all_stds = []\n",
    "\n",
    "#     # Flatten all scores and std deviations\n",
    "#     for src in mean_csa_data.index:\n",
    "#         for trg in mean_csa_data.columns:\n",
    "#             all_scores.append(abs(mean_csa_data.loc[src, trg]))  # Collect absolute values of scores\n",
    "#             all_stds.append(std_csa_data.loc[src, trg])  # Collect standard deviations\n",
    "\n",
    "#     # Compute mean absolute score and overall standard deviation\n",
    "#     mean_abs = sum(all_scores) / len(all_scores)\n",
    "#     overall_std_dev = (sum((x - np.mean(all_stds)) ** 2 for x in all_stds) / len(all_stds)) ** 0.5\n",
    "\n",
    "#     # Compute STVI\n",
    "#     return overall_std_dev / mean_abs if mean_abs != 0 else 0\n",
    "\n",
    "\n",
    "# stvi_bruteforce = compute_stvi_bruteforce(mean_csa_data, std_csa_data)\n",
    "# stvi_vectorized = compute_stvi_vectorized(mean_csa_data, std_csa_data)\n",
    "\n",
    "# print(stvi_vectorized)\n",
    "# print(stvi_bruteforce)\n",
    "\n",
    "# assert np.isclose(stvi_bruteforce, stvi_vectorized, equal_nan=True), \"STVI results do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv('all_models_runtimes.csv')\n",
    "\n",
    "# # 1. Box Plot of Total Minutes by Model\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(x='model', y='tot_mins', data=df)\n",
    "# plt.title('Box Plot of Total Minutes by Model')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.ylabel('Total Minutes')\n",
    "# plt.xlabel('Model')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 2. Bar Plot of Average Total Minutes by Source\n",
    "# avg_tot_mins_src = df.groupby('src')['tot_mins'].mean().reset_index()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(x='src', y='tot_mins', data=avg_tot_mins_src)\n",
    "# plt.title('Average Total Minutes by Source')\n",
    "# plt.ylabel('Average Total Minutes')\n",
    "# plt.xlabel('Source')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 3. Line Plot of Total Minutes by Stage\n",
    "# avg_tot_mins_stage = df.groupby('stage')['tot_mins'].mean().reset_index()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.lineplot(x='stage', y='tot_mins', data=avg_tot_mins_stage, marker='o')\n",
    "# plt.title('Average Total Minutes by Stage')\n",
    "# plt.ylabel('Average Total Minutes')\n",
    "# plt.xlabel('Stage')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 4. Heatmap of Total Minutes by Source and Target\n",
    "# heatmap_data = df.pivot_table(values='tot_mins', index='src', columns='trg', aggfunc='mean')\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap='viridis')\n",
    "# plt.title('Heatmap of Average Total Minutes by Source and Target')\n",
    "# plt.ylabel('Source')\n",
    "# plt.xlabel('Target')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv('all_models_runtimes.csv')\n",
    "\n",
    "# # Group by src, stage, and model, and calculate the mean and standard deviation of tot_mins\n",
    "# stage_model_src_stats = df.groupby(['src', 'stage', 'model'])['tot_mins'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# # Calculate the standard error of the mean (sem)\n",
    "# stage_model_src_stats['sem'] = stage_model_src_stats['std'] / stage_model_src_stats['count'] ** 0.5\n",
    "\n",
    "# # Define a color palette\n",
    "# colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# # Create separate plots for each stage\n",
    "# stages = stage_model_src_stats['stage'].unique()\n",
    "\n",
    "# for stage in stages:\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "#     stage_data = stage_model_src_stats[stage_model_src_stats['stage'] == stage]\n",
    "    \n",
    "#     bar_plot = sns.barplot(x='src', y='mean', hue='model', data=stage_data, palette=colors, errorbar=None)\n",
    "    \n",
    "#     # Add error bars for each bar\n",
    "#     for index, bar in enumerate(bar_plot.patches):\n",
    "#         height = bar.get_height()\n",
    "#         sem = stage_data['sem'].iloc[index]\n",
    "        \n",
    "#         plt.errorbar(x=bar.get_x() + bar.get_width() / 2, \n",
    "#                      y=height, \n",
    "#                      yerr=sem, \n",
    "#                      fmt='none', \n",
    "#                      c='black', \n",
    "#                      capsize=5, \n",
    "#                      elinewidth=1)\n",
    "\n",
    "#     plt.title(f'Distribution of Total Minutes for Stage {stage} with Error Bars')\n",
    "#     plt.ylabel('Average Total Minutes')\n",
    "#     plt.xlabel('Source')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.legend(title='Model')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv('all_models_runtimes.csv')\n",
    "\n",
    "# # Group by src, stage, and model, and calculate the mean and standard deviation of tot_mins\n",
    "# stage_model_src_stats = df.groupby(['src', 'stage', 'model'])['tot_mins'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# # Create separate box plots for each stage\n",
    "# stages = stage_model_src_stats['stage'].unique()\n",
    "\n",
    "# # Define a color palette\n",
    "# colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# for stage in stages:\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "#     stage_data = df[df['stage'] == stage]  # Filter data for the current stage\n",
    "    \n",
    "#     # Create a box plot\n",
    "#     sns.boxplot(x='src', y='tot_mins', hue='model', data=stage_data, palette=colors)\n",
    "    \n",
    "#     plt.title(f'Distribution of Total Minutes for Stage: {stage}')\n",
    "#     plt.ylabel('Total Minutes')\n",
    "#     plt.xlabel('Source')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.legend(title='Model')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ovarian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
